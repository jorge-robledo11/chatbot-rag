import asyncio
from backend.src.infrastructure import get_infrastructure
from backend.src.services.scraper_service import ScraperService
from loguru import logger
from dotenv import load_dotenv, find_dotenv
import os

load_dotenv(find_dotenv())

async def main():
    """
    Orquesta el pipeline de scraping e indexaci√≥n web siguiendo estos pasos:
    1.  Inicializaci√≥n: Configura los clientes de infraestructura y servicios de scraping.
    2.  Extracci√≥n Web: Ejecuta el scraping de fuentes predefinidas con reglas de filtrado.
    3.  Procesamiento: Convierte los datos scraped a documentos estructurados con IDs determin√≠sticos.
    4.  Enriquecimiento: Genera embeddings vectoriales para el contenido de cada documento.
    5.  Indexaci√≥n: Sube los documentos enriquecidos al √≠ndice de b√∫squeda web de Azure.
    6.  Finalizaci√≥n: Cierra conexiones externas para liberar recursos.
    """
    # ------------------------------------------------------------------------------------------
    #                           üîß FASE 1: INICIALIZACI√ìN DE DEPENDENCIAS üîß
    # ------------------------------------------------------------------------------------------
    logger.info("üöÄ Iniciando pipeline de scraping e indexaci√≥n web...")
    infra = get_infrastructure()
    
    # Configuraci√≥n del servicio de scraping
    scraper_config = {
        "llm": {
            "api_key": os.getenv("AZURE_OPENAI_API_KEY"),
            "model": "azure_openai/gpt-4o",
            "api_version": os.getenv("AZURE_OPENAI_API_VERSION")
        },
        "verbose": False,
        "headless": True,
    }
    
    # Fuentes y reglas de procesamiento
    sources = {
        "sobre ajover": "https://www.ajover.com/sobre-ajover/",
        "sostenibilidad": "https://www.ajover.com/sostenibilidad/",
        "noticias": "https://www.ajover.com/noticias/",
        "proyectos": "https://www.ajover.com/proyectos/",
        "gana ajover": "https://www.ajover.com/gana-ajover/",
        'match': 'https://www.ajover.com/noticia/crea-match-perfectos-con-la-mejor-calidad-ajover/',
        'aumentar ventas': 'https://www.ajover.com/noticia/aumenta-tus-ventas-con-nuestra-cubierta-termoacustica/',
        'renovar espacios': 'https://www.ajover.com/noticia/renueva-los-espacios-y-que-tu-hogar-sea-tu-lugar-favorito/',
        'construccion sostenible': 'https://www.ajover.com/noticia/ajover-se-une-al-consejo-colombiano-de-construccion-sostenible/'
    }
    mantener = {"Contacto", "Copyright"}
    eliminar = {
        "L√≠nea √âtica", "Politica de tratamiento de datos", 'Bodega Inducol', 
        'Urbanizaci√≥n San Carlos II', 'Suscr√≠bete a nuestro newsletter', 'Links relevantes', 
        'Casa Campestre Delicias', 'Casa Campestre Villa Leo', 'Casa residencial', 
        'Casa Campestre Delicias',
    }
    prioridad = ["sobre_ajover", "sostenibilidad", "noticias", "proyectos", 'gana_ajover']

    # Inicializaci√≥n del scraper con configuraci√≥n
    scraper = ScraperService(
        config=scraper_config,
        sources=sources,
        mantener=mantener,
        eliminar=eliminar,
        prioridad=prioridad
    )

    # ------------------------------------------------------------------------------------------
    #                           üåê FASE 2: EXTRACCI√ìN WEB (SCRAPING) üåê
    # ------------------------------------------------------------------------------------------
    logger.info("üöÄ Ejecutando scraping de fuentes web...")
    dict_preprocesado = await scraper.run_scraper()
    logger.success("‚úÖ Scraping completado. Datos obtenidos: {} p√°ginas", len(dict_preprocesado))

    # ------------------------------------------------------------------------------------------
    #                       üõ†Ô∏è FASE 3: PROCESAMIENTO DE DOCUMENTOS üõ†Ô∏è
    # ------------------------------------------------------------------------------------------
    logger.info("Convirtiendo datos scraped a documentos estructurados...")
    docs = scraper.convertir_a_documentos_web(dict_preprocesado)
    logger.debug("Documentos generados: {}", [doc["id"] for doc in docs])

    # ------------------------------------------------------------------------------------------
    #                       üîç FASE 4: ENRIQUECIMIENTO CON EMBEDDINGS üîç
    # ------------------------------------------------------------------------------------------
    openai_service = await infra.get_openai()
    search_ai = await infra.get_searchai(index_type="web")
    await search_ai.create_index_if_not_exists()

    logger.info("Generando embeddings para {} documentos...", len(docs))
    for doc in docs:
        doc["content_vector"] = await openai_service.get_text_embedding(doc["content"])
    logger.success("‚úÖ Embeddings generados para todos los documentos")

    # ------------------------------------------------------------------------------------------
    #                       üì• FASE 5: INDEXACI√ìN EN AZURE SEARCH üì•
    # ------------------------------------------------------------------------------------------
    logger.info("Subiendo documentos al √≠ndice web...")
    allowed = {"id", "title", "content", "source", "source_url", "content_vector"}
    docs = [{k: v for k, v in d.items() if k in allowed} for d in docs]
    await search_ai.upload_documents_batch(docs)
    logger.success("üéâ Indexaci√≥n completada. Documentos indexados: {}", len(docs))

    # ------------------------------------------------------------------------------------------
    #                           üîå FASE 6: CIERRE DE CONEXIONES üîå
    # ------------------------------------------------------------------------------------------
    logger.info("Liberando recursos...")
    await infra.shutdown()
    logger.success("‚úÖ Pipeline finalizado. Recursos liberados")

if __name__ == "__main__":
    asyncio.run(main())
